{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Run a RAG Agent  \n",
        "This notebook demonstrates how to build a Chroma vector database and use it to enable an augmented LLM call.  \n",
        "We'll start by installing the chromadb vector database, litellm model interface, and pypdf PDF reader software."
      ],
      "metadata": {
        "id": "QM9wJeFE2RqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install chromadb pypdf litellm"
      ],
      "metadata": {
        "id": "unREjg8i3jJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ignore warnings to keep the screen as clean as we can."
      ],
      "metadata": {
        "id": "jwlxQSZN3d0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zsLtprquL9s3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now import the chromadb library for the vector database and the pypdf library to enable reading pdf files. We'll also need the litellm library to connect to our model, and we'll load Google's userdata so we can retrieve our Gemini key.\n"
      ],
      "metadata": {
        "id": "uQgFmcxsMCqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, chromadb\n",
        "from litellm import completion\n",
        "from pypdf import PdfReader\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "6x0Gbzw52zsO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then set up a client handle and create an empty collection. We'll also load our API key and set the model as gemini."
      ],
      "metadata": {
        "id": "PYIg33rH4TtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client     = chromadb.Client()\n",
        "collection = client.create_collection(name=\"handbook\")\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "model = \"gemini/gemini-2.5-flash\""
      ],
      "metadata": {
        "id": "3Ej-uYij4wT0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we have our GAIA_Quality_Manual uploaded into the Colab local files environment.  \n",
        "We'll now read the pdf file in and we'll get a list of pages as \"page\" type objects. We'll iterate through the pages and extract the text, and add each page in text form to our vector database collection."
      ],
      "metadata": {
        "id": "yJ0IeLDm4yZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGoYAoRH2Pkf"
      },
      "outputs": [],
      "source": [
        "reader = PdfReader(\"GAIA_Quality_Manual.pdf\")\n",
        "\n",
        "texts=[]\n",
        "metas=[]\n",
        "ideas=[]\n",
        "for page in reader.pages:\n",
        "    texts.append(page.extract_text())\n",
        "    metas.append({\"source\":\"page\"+str(len(texts))})\n",
        "    ideas.append(\"idea\"+str(len(texts)))\n",
        "    collection.add(\n",
        "        documents = texts,\n",
        "        metadatas = metas,\n",
        "        ids       = ideas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up the system instructions for the AI model that we'll call and the user prompt."
      ],
      "metadata": {
        "id": "Nj2B0TfK5ZGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "   You are a helpful business architect who can identify business requirements.  You analyze the provided Context\n",
        "   and select requirements which exist in the Context in order to answer the user Question This is the only place\n",
        "   the requirements exist so do not use your own knowledge to answer the question, use only the Context.\n",
        "   Attributes are single word adjectives describing a requirement. Identify any key performance attributes\n",
        "   related to the requirements and the list of single word attributes that relate to them.\n",
        "   You always return a json string in the following form:\n",
        "   { 'requirement':'requirement 1','attribute_list':['attribute 1','attribute 2',..],..}\n",
        "   If you cannot find the answer in the Context then reply that you do not know.\n",
        "   \"\"\"\n",
        "\n",
        "query = \"What skills should Laboratory employees have?\""
      ],
      "metadata": {
        "id": "lmXzcdhe5dBN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to build our context. We'll get back a set of documents, iterate through them., and build a text-based context."
      ],
      "metadata": {
        "id": "r4h7NWuk_ENv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection.query(query_texts=[query],n_results=5)\n",
        "docs = results[\"documents\"]\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    context = context.join(doc[0])\n",
        "print(\"Context length is \"+str(len(context)))"
      ],
      "metadata": {
        "id": "cFVmLlv-ADaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now build our LLM user prompt, call the model, and print the response. We'll set the LLM temperature to 0.5 for a fairly constrained response."
      ],
      "metadata": {
        "id": "P_m1EPbzBUBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt  = f\"\"\"\n",
        "   Answer the question based on the Context.\n",
        "   Context:\n",
        "   {context}\n",
        "   Question:\n",
        "   {query}\n",
        "   \"\"\"\n",
        "response = completion(\n",
        "              model = model,\n",
        "              messages = [\n",
        "                {\"role\":\"system\",\"content\":system_instructions},\n",
        "                {\"role\":\"user\",\"content\":prompt}\n",
        "              ],\n",
        "              temperature=0.5,\n",
        "           )\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "mUOPCYUzCxvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}