{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AGNO Secure Agent\n",
        "This notebook demonstrates how to use the Agno agentic framework to make a RAG call to Gemini with a prompt injection guardrail. We'll start by installing the agno library"
      ],
      "metadata": {
        "id": "EjpIsUXnvFP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O46jZf9iu6Wg"
      },
      "outputs": [],
      "source": [
        "%pip install agno chromadb pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now import the Agno functions including the connector to connect to Gemini. We'll also include the pipeline for our huggingface guardrail."
      ],
      "metadata": {
        "id": "nsUSJARRvLuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import chromadb\n",
        "from pypdf import PdfReader\n",
        "from agno.agent import Agent\n",
        "from agno.models.google import Gemini\n",
        "from agno.exceptions import InputCheckError\n",
        "from agno.guardrails import BaseGuardrail\n",
        "from agno.run.agent import RunInput\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyjYKic2vRXH",
        "outputId": "bfb8dd0b-c936-494b-d4c1-3cb9b1c2c51e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create our vector database and preload the pdf document.  make sure the document has been uploaded to Colab before running this cell."
      ],
      "metadata": {
        "id": "hNjb4AmoZyTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client     = chromadb.Client()\n",
        "collection = client.create_collection(name=\"handbook\")\n",
        "reader = PdfReader(\"GAIA_Quality_Manual.pdf\")\n",
        "\n",
        "texts=[]\n",
        "metas=[]\n",
        "ideas=[]\n",
        "for page in reader.pages:\n",
        "    texts.append(page.extract_text())\n",
        "    metas.append({\"source\":\"page\"+str(len(texts))})\n",
        "    ideas.append(\"idea\"+str(len(texts)))\n",
        "    collection.add(\n",
        "        documents = texts,\n",
        "        metadatas = metas,\n",
        "        ids       = ideas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwr4_si4ZzE7",
        "outputId": "a56118c5-3e7d-4a85-a8e7-8ec327f473e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:01<00:00, 73.8MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a pipeline handle for the ProtectAI Deberta prompt injection guardrail."
      ],
      "metadata": {
        "id": "MaxdLXWXBKQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe1 = pipeline(\"text-classification\", model=\"protectai/deberta-v3-base-prompt-injection-v2\")"
      ],
      "metadata": {
        "id": "RrrSH-vuBTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define the pre AI model guardrail prompt injection function as a class. We'll use Agno's custom tool approach, and we need to define both sync and async calling methods."
      ],
      "metadata": {
        "id": "DFLf2v6FvpD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreGuardrail(BaseGuardrail):\n",
        "    def check(self, run_input: RunInput) -> None:\n",
        "        if isinstance(run_input.input_content, str):\n",
        "              if pipe1(run_input.input_content)[0]['label']== \"INJECTION\":\n",
        "                 raise InputCheckError(\"User appears to be attempting a prompt injection.\")\n",
        "    async def async_check(self, run_input: RunInput) -> None:\n",
        "        self.check(run_input)"
      ],
      "metadata": {
        "id": "FcWYF0cOvp6i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our system instructions for Gemini."
      ],
      "metadata": {
        "id": "1oKuTdJnbBFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "   You are a helpful business architect who can identify business requirements.  You analyze the provided Context\n",
        "   and select requirements which exist in the Context in order to answer the user Question This is the only place\n",
        "   the requirements exist so do not use your own knowledge to answer the question, use only the Context.\n",
        "   Attributes are single word adjectives describing a requirement. Identify any key performance attributes\n",
        "   related to the requirements and the list of single word attributes that relate to them.\n",
        "   You always return a json string in the following form:\n",
        "   { 'requirement':'requirement 1','attribute_list':['attribute 1','attribute 2',..],..}\n",
        "   If you cannot find the answer in the Context then reply that you do not know.\n",
        "   \"\"\""
      ],
      "metadata": {
        "id": "xDksNzdTagRE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now build the agent using Agno and specify that the LLM we'll use is gemini-2.5-flash, and we'll use our pre-model guardrail."
      ],
      "metadata": {
        "id": "ZRm_YOjVvXIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preguard = PreGuardrail()\n",
        "agent = Agent(\n",
        "    model=Gemini(id=\"gemini-2.5-flash\",api_key=userdata.get(\"GEMINI_API_KEY\")),\n",
        "    instructions=[system_instructions],\n",
        "    description=\"You are a helpful research assistant\",\n",
        "    pre_hooks=[preguard],\n",
        "    markdown=True\n",
        ")"
      ],
      "metadata": {
        "id": "yq9Y-w9vvWN6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll pose a question that we'll have the agent answer."
      ],
      "metadata": {
        "id": "0yPgbgR3cKL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What skills should Laboratory employees have?\""
      ],
      "metadata": {
        "id": "SsuYekECcSk-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to get our context and construct our rich context-enhanced prompt."
      ],
      "metadata": {
        "id": "HA-hWbq8aWCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = collection.query(query_texts=[query],n_results=5)\n",
        "docs = results[\"documents\"]\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    context = context.join(doc[0])\n",
        "\n",
        "prompt  = f\"\"\"\n",
        "   Answer the question based on the Context.\n",
        "   Context:\n",
        "   {context}\n",
        "   Question:\n",
        "   {query}\n",
        "   \"\"\""
      ],
      "metadata": {
        "id": "UPrEa9xVbl-r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now make a request and stream the result out for display."
      ],
      "metadata": {
        "id": "r0sMh-BSvgGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.print_response(prompt, stream=True)"
      ],
      "metadata": {
        "id": "Ymw65aDkvj_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}