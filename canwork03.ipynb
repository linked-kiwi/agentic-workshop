{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP4q93LXRw4XCAKb93OdyRH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Programmatic Access to AI Models\n","In this notebook we'll learn how to access the free-tier Google AI models and also to download and run huggingface models.  Make sure at the top right you change the runtime type to the T4 GPU."],"metadata":{"id":"Ojt-pG60aq8P"}},{"cell_type":"markdown","source":["The first thing we'll do is switch off warnings to remove unecessary noise when we're running the notbook."],"metadata":{"id":"H0o3Aq9QbCvC"}},{"cell_type":"code","source":["from warnings import filterwarnings\n","filterwarnings('ignore')"],"metadata":{"id":"QT8V9rI8bPnV","executionInfo":{"status":"ok","timestamp":1738264229352,"user_tz":-780,"elapsed":575,"user":{"displayName":"Malcolm Shore","userId":"03653801920052420148"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Google Gemini 1.5  \n","We can use Google's Gemini 1.5 model on the free tier. Note that we also have in Google Colab the option to set our running environment to be eitehr CPU or a T4 GPU. Having the GPU available is a powerful capability for running agents."],"metadata":{"id":"wJYNgmyzbX3B"}},{"cell_type":"markdown","source":["We need to use the Google generativeai library. This is already available in the Colab environment so we don't need to install it. We do, however, need to install our Google API key in the Colab vault and then use it to configure our access."],"metadata":{"id":"HoO4EQG7cJ3x"}},{"cell_type":"code","source":["import google.generativeai as genai\n","from google.colab import userdata\n","\n","genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"],"metadata":{"id":"nyW6poh-clwD","executionInfo":{"status":"ok","timestamp":1738261468467,"user_tz":-780,"elapsed":9166,"user":{"displayName":"Malcolm Shore","userId":"03653801920052420148"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Let's now set up a handle specifying the model we want to use.  Google offers the smaller Gemma model and its full Gemini models. The Gemini 1.5 is available in the free tier, and there are a range of more advanced Gemini 2.0 models that can be used with the paid service."],"metadata":{"id":"ieUCXCy4dSRg"}},{"cell_type":"code","source":["model = genai.GenerativeModel(\"gemini-1.5-flash\")"],"metadata":{"id":"ubXavC_FdmDO","executionInfo":{"status":"ok","timestamp":1738261795365,"user_tz":-780,"elapsed":343,"user":{"displayName":"Malcolm Shore","userId":"03653801920052420148"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["We can now send Gemini a request"],"metadata":{"id":"g1zpcr-peGRE"}},{"cell_type":"code","source":["response = model.generate_content(\"What is the gravity on the surface of Mars relative to Earth's gravity?\")"],"metadata":{"id":"fJxsV2iDeJil","executionInfo":{"status":"ok","timestamp":1738261854668,"user_tz":-780,"elapsed":3400,"user":{"displayName":"Malcolm Shore","userId":"03653801920052420148"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["We have our response so let's print it."],"metadata":{"id":"wKKbNe_6exd2"}},{"cell_type":"code","source":["print(response.text)"],"metadata":{"id":"uzrFh2xwe1Re"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Huggingface Qwen2.5\n"],"metadata":{"id":"jPeARmr-e-To"}},{"cell_type":"markdown","source":["We can use the Huggingface transformers library to directly download and run any of the models on Huggingface. Qwen is a standard family of generative AI models available on Huggingface.  Note for this we need to have HF_TOKEN set up in Colab's vault."],"metadata":{"id":"BTr-a_ErfJwT"}},{"cell_type":"markdown","source":["Colab comes with the tranformers library already installed, so we can directly import our pipeline for loading and operating a model, and use it to create a pipe for the Qwen2.5 model. We'll use the model with 1.5 billion parameters. We'll set the device to use the CUDA library for GPU operation."],"metadata":{"id":"vqTiaN9ZiyD1"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","pipe = pipeline(\"text-generation\",model=\"Qwen/Qwen2.5-1.5B\",device=\"cuda\")"],"metadata":{"id":"O4220K6mi5p6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see we've set the device type that we'll use for running the model. cuda is much faster than cpu!  Now let's send in our prompt and display the response."],"metadata":{"id":"Ji5ZAPFqm-f1"}},{"cell_type":"code","source":["response = pipe(\"What is the gravity on Mars relative to that on Earth?\")\n","print(response[0]['generated_text'])"],"metadata":{"id":"1NLJL9GjnNuU"},"execution_count":null,"outputs":[]}]}