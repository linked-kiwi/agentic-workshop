{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d26a546-4e2d-4cd7-a8b0-d35bea779ce0",
   "metadata": {
    "id": "7d26a546-4e2d-4cd7-a8b0-d35bea779ce0"
   },
   "source": [
    "## Basic coding for AI systems\n",
    "\n",
    "We work with AI models using text, but internally the models use what are called \"tokens\" to represent the basic atoms of processing (we'll focus on words). Let's see how we would use a large text base to create a tokenizer.  We'll use the collected works of Jane Austen. The tokenizer code is taken from the examples provided by Lightening AI.  \n",
    "  \n",
    "We'll ignore warnings as we run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac37f11e-9780-4152-a59c-fc2c88946d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gL2GPtqMV78h",
   "metadata": {
    "id": "gL2GPtqMV78h"
   },
   "source": [
    "We create a class from which we can call the encode and decode functions, which turn words into tokens and tokens into words. At this step we're not using these functions, but we are preparing the dictionary vocab with which they function. For the dictionary we'll use janes.txt, which comes from the consolidated works of Jane Austen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28863197-ca83-4dcd-ab58-99b35f38087d",
   "metadata": {
    "id": "28863197-ca83-4dcd-ab58-99b35f38087d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Tokenizer:\n",
    "  def __init__(self,vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "  def encode(self,text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "  def decode(self,ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9d313-6431-4e4e-8241-a68e85126c62",
   "metadata": {},
   "source": [
    "## Building a dictionary\n",
    "Now that we have the tokenizer functions (methods) defined, we will read the textual version of Jane Austen's works and turn them into a dictionary assigning a token value for each unique word. Once complete, we'll display the size of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66091bf0-6b60-4c88-8876-9d96df0ea746",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"janes.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "  raw_text=f.read()\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed = [item for item in preprocessed if item]\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab={token:integer for integer,token in enumerate(all_words)}\n",
    "print(\"Vocabulary size: \",vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90717eeb-a57b-4878-b353-94fb4baf8dcc",
   "metadata": {
    "id": "90717eeb-a57b-4878-b353-94fb4baf8dcc"
   },
   "source": [
    "## Tokenizing and recovering text\n",
    "\n",
    "Now that we have a dictionary we can use it to turn a word into a token, and recover a word from a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793c559-004c-40ce-bd35-e199ece0ef3d",
   "metadata": {
    "id": "5793c559-004c-40ce-bd35-e199ece0ef3d"
   },
   "outputs": [],
   "source": [
    "# ...following on\n",
    "tokenizer=Tokenizer(vocab)\n",
    "\n",
    "text=\"It is a truth universally acknowledged\"\n",
    "\n",
    "ids=tokenizer.encode(text)\n",
    "print(\"Token stream: \",ids)\n",
    "\n",
    "print(\"Recovery:     \",tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7aa0cb-2895-4466-b85c-734665b15ef5",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Embeddings are vectors that capture the semantic context of text. They are generated by embedding models that learn from vast amounts of text data, tokenizing it and establishing its relationships with other tokens.  To demonstrate this, we'll use the sentence_transformers library and call the MiniLM-L6 model.  We'll embed the previous text phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8a573-e35d-4521-809b-82ff984639aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q sentence_transformers\n",
    "%pip install -q tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6d37a-4669-40fc-98d8-1c184a55c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode([text])\n",
    "print(\"Embedding structure size: \",embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2348f83-a276-493f-a9ce-4d54d66ae2c0",
   "metadata": {},
   "source": [
    "We can dump out the full embedding structure to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ad0fa-9800-441f-8a2d-83e002026dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full embedding structure:\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0e306-ae1e-4cbd-8627-1d105bab09eb",
   "metadata": {},
   "source": [
    "Great - we've successfully embedded a prompt."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1BcjaPkVSdZ-Rdsvw2hyRnKThFiXmqgA1",
     "timestamp": 1727171387883
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
