{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs69yXrHuPSB"
   },
   "source": [
    "## Programmatic Execution of GenAI Models\n",
    "An agentic system consists of one or more AI agents and one or more tools that can be used by the AI agent(s). A typical simple tool is an AI agent that uses a tool to browse the internet. Execution of an AI model is one component of an Agentic system.  \n",
    "\n",
    "In this notebook we'll demonstrate how we interface to online and local APIs. We will not be using these in our agent build in this workshop, but you may wish to use them when you build your own agent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll suppress warnings as they create a lot of noise when running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1738234324520,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "omy-SsZM1f_R"
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj3MyXlRvUdr"
   },
   "source": [
    "### OpenAI Online Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THPvHXam1Y_5"
   },
   "source": [
    "The first appropach to programmatic execution of an AI agent that we'll demonstrate is the connection to OpenAI's gpt service. This is the programatic equivalent of running ChatGPT. To run this we need an OpenAI API key, which is a paid service from OpenAI. While the cost of running OpenAI for this workshop is typically less than $0.50, we'll be using community and open source solutions for our agentic build.  This demonstration is 'for-interest' only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll install the openai python library so that we can use it in our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2999,
     "status": "ok",
     "timestamp": 1738234352843,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "xksX2gk2u_kU"
   },
   "outputs": [],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wmDJ1q6uOnp"
   },
   "source": [
    "We can now import the openai library.  This is where we need to use our paid OpenAI API key. I have mine loaded already as an environment variable so we'll load it and then use it to set up a handle for accessing OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9561,
     "status": "ok",
     "timestamp": 1738233699251,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "BUC7KZF0vbCe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsPdz_zMw5dx"
   },
   "source": [
    "We can now call the OpenAI service to run an AI model and get a response to the prompt we give it.  We'll select the full gpt-4o model for our use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3630,
     "status": "ok",
     "timestamp": 1738234086509,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "SuMyQ_STyNbr"
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "   model=\"gpt-4o\",\n",
    "   messages=[\n",
    "      { \"role\":\"system\",\"content\":\"You are a helpful assistant\"},\n",
    "      { \"role\":\"user\",\"content\":\"How far from earth is the nearest exoplanet?\"}\n",
    "   ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wS0KNWJy3Gn"
   },
   "source": [
    "We've got our answer from gpt-4o, so let's display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsZW3xYZy-W8"
   },
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wS3r-RH2M3M"
   },
   "source": [
    "### Ollama Local AI Model  \n",
    "Ollama is a great open source solution, but it is fairly complex trying to install and run it in the Colab environment. Given we're using Colab, we'll not use ollama for our agent. However, you may wish to follow up this workshop with your own agent builds, and ollama is a great service to use. In this section we'll use the local ollama service running on the laptop to run the mistral model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is install the ollama library so that we can use it in our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3200,
     "status": "ok",
     "timestamp": 1738234914647,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "jAsgzh-D2jkH"
   },
   "outputs": [],
   "source": [
    "%pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLhCjgTV2rom"
   },
   "source": [
    "We'll now import the ollama library. We don't need an API key and we don't need to set up a client handle as we can make a call directly using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1738234920354,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "wH64W9yd3IxZ"
   },
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WWhjf_i3LUD"
   },
   "source": [
    "We can now directly call mistral through the ollama interface. If we haven't got the mistral model loaded, ollama will load it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "error",
     "timestamp": 1738234989941,
     "user": {
      "displayName": "Malcolm Shore",
      "userId": "03653801920052420148"
     },
     "user_tz": -780
    },
    "id": "QAdedfv03Ur1",
    "outputId": "e36fd157-51e6-4307-8a93-d9d7b990f2c0"
   },
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[\n",
    "        { \"role\":\"user\",\"content\":\"How far from earth is the nearest exoplanet\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTZpdKew3xtP"
   },
   "source": [
    "We can now extract the answer from our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZitgEu-327X"
   },
   "outputs": [],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfhEiBCuPJRFrEzi5pQWmS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
